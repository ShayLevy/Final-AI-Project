"""
LLM-as-a-Judge Evaluation System
Uses a SEPARATE model (Anthropic Claude) to evaluate system performance.

This ensures independent evaluation - the judge model is different from the
model used for generating answers (OpenAI GPT-4), reducing evaluation bias.

Requires ANTHROPIC_API_KEY environment variable to be set.
"""

import os
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from typing import Dict, Any, List
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMJudge:
    """
    LLM-as-a-Judge evaluation system using Anthropic Claude.

    Uses Claude as an independent judge to evaluate the RAG system's responses,
    which are generated by OpenAI GPT-4. This separation ensures unbiased evaluation.

    Evaluates: Answer Correctness, Context Relevancy, Context Recall
    """

    def __init__(self, judge_model: str = "claude-3-5-haiku-20241022", temperature: float = 0):  # Changed from sonnet-4 to haiku (12x cheaper!)
        """
        Initialize LLM Judge with Anthropic Claude

        Args:
            judge_model: Anthropic model to use for evaluation (default: claude-sonnet-4-20250514)
            temperature: Temperature setting (0 for deterministic)

        Raises:
            ValueError: If ANTHROPIC_API_KEY environment variable is not set
        """
        # Check for Anthropic API key
        if not os.getenv("ANTHROPIC_API_KEY"):
            raise ValueError(
                "ANTHROPIC_API_KEY not found in environment variables. "
                "Please set it in your .env file. "
                "Get your key from: https://console.anthropic.com/"
            )

        self.llm = ChatAnthropic(model=judge_model, temperature=temperature)
        self.model_name = judge_model
        logger.info(f"LLMJudge initialized with Anthropic model: {judge_model}")
        logger.info("Using separate model for evaluation (Claude) vs generation (GPT-4)")

    def evaluate_correctness(
        self,
        query: str,
        answer: str,
        ground_truth: str
    ) -> Dict[str, Any]:
        """
        Evaluate answer correctness against ground truth

        Args:
            query: Original question
            answer: System's answer
            ground_truth: Expected correct answer

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating answer correctness...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how accurately the answer matches the ground truth.

Scoring (1-5):
5 = Perfect match, all key facts correct
4 = Mostly correct, minor missing details
3 = Partially correct, some key facts present
2 = Minimally correct, few facts match
1 = Incorrect, facts don't match

Consider:
- Factual accuracy (dates, numbers, names)
- Completeness of information
- Absence of contradictions

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "key_facts_matched": ["fact1", "fact2"], "key_facts_missed": ["fact3"]}}"""),
            ("human", """Query: {query}

Ground Truth: {ground_truth}

System Answer: {answer}

Evaluate the correctness:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "ground_truth": ground_truth,
            "answer": answer
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)
            return {
                "metric": "correctness",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "matched_facts": result.get("key_facts_matched", []),
                "missed_facts": result.get("key_facts_missed", [])
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "correctness")

    def evaluate_relevancy(
        self,
        query: str,
        retrieved_context: str
    ) -> Dict[str, Any]:
        """
        Evaluate context relevancy - did the system retrieve relevant information?

        Args:
            query: Original question
            retrieved_context: Context retrieved by the system

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating context relevancy...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how relevant the retrieved context is to the query.

Scoring (1-5):
5 = Highly relevant, directly addresses query
4 = Mostly relevant, contains answer with some extra info
3 = Partially relevant, some useful information
2 = Minimally relevant, mostly unrelated
1 = Irrelevant, doesn't help answer query

Consider:
- Does context contain information to answer the query?
- Is the context specific or too broad?
- Is there unnecessary information?

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "relevant_portions": ["portion1", "portion2"], "irrelevant_portions": ["portion3"]}}"""),
            ("human", """Query: {query}

Retrieved Context:
{context}

Evaluate the relevancy:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "context": retrieved_context
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)
            return {
                "metric": "relevancy",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "relevant_portions": result.get("relevant_portions", []),
                "irrelevant_portions": result.get("irrelevant_portions", [])
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "relevancy")

    def evaluate_recall(
        self,
        query: str,
        expected_chunks: List[str],
        retrieved_chunks: List[str]
    ) -> Dict[str, Any]:
        """
        Evaluate context recall - did the system retrieve all necessary chunks?

        Args:
            query: Original question
            expected_chunks: Descriptions of chunks that should be retrieved
            retrieved_chunks: Chunks actually retrieved

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating context recall...")

        # Calculate basic recall
        if not expected_chunks:
            recall_percentage = 100.0
        else:
            matches = 0
            for expected in expected_chunks:
                for retrieved in retrieved_chunks:
                    if expected.lower() in retrieved.lower() or retrieved.lower() in expected.lower():
                        matches += 1
                        break
            recall_percentage = (matches / len(expected_chunks)) * 100

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how well the system retrieved all necessary information.

Scoring (1-5):
5 = All necessary chunks retrieved
4 = Most necessary chunks retrieved
3 = Some necessary chunks retrieved
2 = Few necessary chunks retrieved
1 = Missed most/all necessary chunks

Consider:
- Were all relevant document sections accessed?
- Is any critical information missing?

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "recall_percentage": <0-100>, "retrieved_expected": ["item1"], "missed_expected": ["item2"]}}"""),
            ("human", """Query: {query}

Expected Chunks (what should be retrieved):
{expected}

Actually Retrieved Chunks:
{retrieved}

Evaluate the recall:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "expected": "\n".join([f"- {chunk}" for chunk in expected_chunks]),
            "retrieved": "\n".join([f"- {chunk[:200]}..." for chunk in retrieved_chunks])
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)
            return {
                "metric": "recall",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "recall_percentage": recall_percentage,
                "retrieved_expected": result.get("retrieved_expected", []),
                "missed_expected": result.get("missed_expected", [])
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, using calculated recall")
            return {
                "metric": "recall",
                "score": int(recall_percentage / 20) + 1,  # Convert to 1-5 scale
                "reasoning": f"Calculated recall: {recall_percentage:.1f}%",
                "recall_percentage": recall_percentage,
                "retrieved_expected": [],
                "missed_expected": []
            }

    # ==========================================
    # NEW MODEL-BASED GRADERS (7 Additional)
    # ==========================================

    def evaluate_faithfulness(
        self,
        answer: str,
        retrieved_context: str
    ) -> Dict[str, Any]:
        """
        Evaluate faithfulness - answer grounded in retrieved context.
        Separate from correctness: checks if claims are supported by context.

        Args:
            answer: System's answer
            retrieved_context: Context retrieved by system

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating faithfulness...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how well the answer is grounded in the retrieved context.

Scoring (1-5):
5 = All claims fully supported by context, no hallucinations
4 = Mostly faithful, minor unsupported details (e.g., formatting differences)
3 = Partially faithful, some claims lack support but core facts correct
2 = Multiple unsupported claims, context contradicts some statements
1 = Severe hallucinations, answer conflicts with retrieved context

Consider:
- Does every factual claim have support in the context?
- Are there contradictions between answer and context?
- Are specific values (dates, amounts, names) correctly quoted?

ESCAPE CLAUSE: If context is empty or irrelevant, return score "N/A".

Respond in JSON format:
{{"score": <1-5 or "N/A">, "reasoning": "<explanation>", "supported_claims": ["claim1"], "unsupported_claims": ["claim2"], "contradictions": []}}"""),
            ("human", """Retrieved Context:
{context}

Answer:
{answer}

Evaluate faithfulness:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "context": retrieved_context,
            "answer": answer
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            score = result.get("score", 0)
            escape_clause_triggered = (score == "N/A")

            return {
                "metric": "faithfulness",
                "score": score if not escape_clause_triggered else "N/A",
                "reasoning": result.get("reasoning", ""),
                "supported_claims": result.get("supported_claims", []),
                "unsupported_claims": result.get("unsupported_claims", []),
                "contradictions": result.get("contradictions", []),
                "escape_clause_triggered": escape_clause_triggered
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "faithfulness")

    def evaluate_helpfulness(
        self,
        query: str,
        answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate helpfulness - answer can be correct but unhelpful.
        Separate from correctness: focuses on user utility.

        Args:
            query: Original question
            answer: System's answer

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating helpfulness...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how helpful the answer is to the user.

Scoring (1-5):
5 = Comprehensive, directly answers question with context. User needs no follow-up
4 = Helpful, answers question but could provide more context or examples
3 = Partially helpful, answers question but misses important nuances
2 = Minimally helpful, answers question but lacks essential context
1 = Unhelpful, answer is confusing, incomplete, or requires clarification

Consider:
- Does answer directly address the question?
- Is context provided when needed?
- Is answer actionable/usable?
- Are important caveats mentioned?

ESCAPE CLAUSE: If answer is factually wrong, return score "N/A" (correctness failure, not helpfulness).

Respond in JSON format:
{{"score": <1-5 or "N/A">, "reasoning": "<explanation>", "strengths": ["strength1"], "weaknesses": ["weakness1"]}}"""),
            ("human", """Question: {query}

Answer: {answer}

Evaluate helpfulness:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "answer": answer
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            score = result.get("score", 0)
            escape_clause_triggered = (score == "N/A")

            return {
                "metric": "helpfulness",
                "score": score if not escape_clause_triggered else "N/A",
                "reasoning": result.get("reasoning", ""),
                "strengths": result.get("strengths", []),
                "weaknesses": result.get("weaknesses", []),
                "escape_clause_triggered": escape_clause_triggered
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "helpfulness")

    def evaluate_coherence(
        self,
        answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate coherence - logical structure and flow.
        Separate from correctness: answer can be factually correct but incoherent.

        Args:
            answer: System's answer

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating coherence...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate the logical coherence and structure of the answer.

Scoring (1-5):
5 = Perfectly structured, logical flow, no contradictions, easy to follow
4 = Mostly coherent, minor flow issues but understandable
3 = Partially coherent, some logical gaps or unclear transitions
2 = Minimally coherent, disjointed information, hard to follow
1 = Incoherent, contradictory statements, confusing structure

Consider:
- Is information presented in logical order?
- Are there internal contradictions?
- Do sentences flow naturally?
- Is structure clear (e.g., chronological for timelines)?

IGNORE factual accuracy (graded separately).

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "structure_quality": "good|fair|poor", "contradictions_found": [], "flow_issues": []}}"""),
            ("human", """Answer: {answer}

Evaluate coherence:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({"answer": answer})

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            return {
                "metric": "coherence",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "structure_quality": result.get("structure_quality", "unknown"),
                "contradictions_found": result.get("contradictions_found", []),
                "flow_issues": result.get("flow_issues", [])
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "coherence")

    def evaluate_conciseness(
        self,
        query: str,
        answer: str,
        query_type: str = "needle"
    ) -> Dict[str, Any]:
        """
        Evaluate conciseness - context-dependent (needle vs summary queries).

        Args:
            query: Original question
            answer: System's answer
            query_type: "needle" or "summary" (affects rubric)

        Returns:
            Dictionary with score and reasoning
        """
        logger.info(f"Evaluating conciseness ({query_type} query)...")

        if query_type == "needle":
            rubric = """5 = Precise, direct answer. No unnecessary information
4 = Mostly concise, minor extra details
3 = Somewhat verbose, answer buried in extra text
2 = Very verbose, answer hard to find
1 = Excessively verbose, overwhelming detail"""
        else:  # summary
            rubric = """5 = Comprehensive yet focused. No fluff
4 = Good coverage, minor redundancies
3 = Some unnecessary details, but complete
2 = Too verbose or too sparse
1 = Extremely verbose or missing key info"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""You are an evaluation judge. Rate answer conciseness relative to query type.

Query Type: {query_type}

Scoring (1-5):
{rubric}

For needle queries: Is answer precise and direct?
For summary queries: Is answer comprehensive without fluff?

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "answer_length": <word_count>, "appropriate_for_query_type": true|false, "verbosity_issues": []}}"""),
            ("human", """Query: {query}

Answer: {answer}

Evaluate conciseness:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "answer": answer
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            return {
                "metric": "conciseness",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "answer_length": result.get("answer_length", len(answer.split())),
                "appropriate_for_query_type": result.get("appropriate_for_query_type", True),
                "verbosity_issues": result.get("verbosity_issues", []),
                "query_type": query_type
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "conciseness")

    def evaluate_citation_quality(
        self,
        answer: str,
        retrieved_context: str
    ) -> Dict[str, Any]:
        """
        Evaluate citation quality - are sources cited correctly?

        Args:
            answer: System's answer
            retrieved_context: Context retrieved by system

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating citation quality...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate the quality of source citations in the answer.

Scoring (1-5):
5 = All facts cited with section references. Citations accurate
4 = Most facts cited, minor citation gaps
3 = Some facts cited, significant gaps
2 = Few citations, mostly uncited claims
1 = No citations or incorrect citations

Consider:
- Are factual claims cited (e.g., "per Medical Documentation")?
- Are citations accurate (info actually in cited section)?
- Are citations specific enough?

ESCAPE CLAUSE: If context has no section metadata, return score "N/A".

Respond in JSON format:
{{"score": <1-5 or "N/A">, "reasoning": "<explanation>", "cited_facts": <count>, "uncited_facts": <count>, "incorrect_citations": []}}"""),
            ("human", """Answer: {answer}

Retrieved Context: {context}

Evaluate citation quality:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "answer": answer,
            "context": retrieved_context
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            score = result.get("score", 0)
            escape_clause_triggered = (score == "N/A")

            return {
                "metric": "citation_quality",
                "score": score if not escape_clause_triggered else "N/A",
                "reasoning": result.get("reasoning", ""),
                "cited_facts": result.get("cited_facts", 0),
                "uncited_facts": result.get("uncited_facts", 0),
                "incorrect_citations": result.get("incorrect_citations", []),
                "escape_clause_triggered": escape_clause_triggered
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "citation_quality")

    def evaluate_query_understanding(
        self,
        query: str,
        answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate query understanding - did agent understand the question?
        Separate from correctness: can understand query but retrieve wrong facts.

        Args:
            query: Original question
            answer: System's answer

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating query understanding...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate how well the agent understood the query.

Scoring (1-5):
5 = Perfect understanding of query intent and requirements
4 = Good understanding, minor misinterpretation
3 = Partial understanding, missed some nuances
2 = Poor understanding, answered wrong question
1 = Complete misunderstanding of query

Consider:
- Did agent address the actual question asked?
- Did agent handle ambiguities appropriately?
- Did agent infer missing context correctly?

SEPARATE from correctness: Agent can understand query perfectly but retrieve wrong facts.

Respond in JSON format:
{{"score": <1-5>, "reasoning": "<explanation>", "query_intent_identified": true|false, "misunderstandings": []}}"""),
            ("human", """Query: {query}

Answer: {answer}

Evaluate query understanding:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "answer": answer
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            return {
                "metric": "query_understanding",
                "score": result.get("score", 0),
                "reasoning": result.get("reasoning", ""),
                "query_intent_identified": result.get("query_intent_identified", False),
                "misunderstandings": result.get("misunderstandings", [])
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "query_understanding")

    def evaluate_efficiency(
        self,
        query: str,
        tool_trace: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate efficiency - optimal tool usage economy.

        Args:
            query: Original question
            tool_trace: List of tools called during answer generation

        Returns:
            Dictionary with score and reasoning
        """
        logger.info("Evaluating efficiency...")

        if not tool_trace:
            return {
                "metric": "efficiency",
                "score": "N/A",
                "reasoning": "Tool trace unavailable",
                "escape_clause_triggered": True
            }

        tool_names = [t.get("tool", "unknown") for t in tool_trace]
        tool_summary = ", ".join(tool_names)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an evaluation judge. Rate the efficiency of agent's tool usage.

Scoring (1-5):
5 = Optimal tool usage. No redundant retrievals
4 = Efficient, minor redundancies
3 = Acceptable, some unnecessary tool calls
2 = Inefficient, multiple redundant retrievals
1 = Highly inefficient, excessive tool usage

Consider:
- Was minimal necessary information retrieved?
- Were tools called in logical order?
- Were there redundant retrievals?

ESCAPE CLAUSE: If tool trace unavailable, return score "N/A".

Respond in JSON format:
{{"score": <1-5 or "N/A">, "reasoning": "<explanation>", "tools_called": <count>, "optimal_tool_count": <count>, "redundant_calls": []}}"""),
            ("human", """Query: {query}

Tool Trace: {tool_trace}

Evaluate efficiency:""")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "tool_trace": tool_summary
        })

        try:
            cleaned_response = self._strip_markdown_code_blocks(response.content)
            result = json.loads(cleaned_response)

            score = result.get("score", 0)
            escape_clause_triggered = (score == "N/A")

            return {
                "metric": "efficiency",
                "score": score if not escape_clause_triggered else "N/A",
                "reasoning": result.get("reasoning", ""),
                "tools_called": result.get("tools_called", len(tool_trace)),
                "optimal_tool_count": result.get("optimal_tool_count", 0),
                "redundant_calls": result.get("redundant_calls", []),
                "escape_clause_triggered": escape_clause_triggered
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON, extracting score manually")
            return self._parse_fallback(response.content, "efficiency")

    # ==========================================
    # END NEW GRADERS
    # ==========================================

    def evaluate_full(
        self,
        query: str,
        answer: str,
        ground_truth: str,
        retrieved_context: str,
        expected_chunks: List[str] = None,
        retrieved_chunks: List[str] = None
    ) -> Dict[str, Any]:
        """
        Perform full evaluation with all metrics

        Args:
            query: Original question
            answer: System's answer
            ground_truth: Expected answer
            retrieved_context: Context retrieved by system
            expected_chunks: Expected chunks to retrieve (optional)
            retrieved_chunks: Actually retrieved chunks (optional)

        Returns:
            Complete evaluation results
        """
        logger.info(f"Performing full evaluation for query: '{query[:50]}...'")

        results = {
            "query": query,
            "correctness": self.evaluate_correctness(query, answer, ground_truth),
            "relevancy": self.evaluate_relevancy(query, retrieved_context)
        }

        if expected_chunks and retrieved_chunks:
            results["recall"] = self.evaluate_recall(query, expected_chunks, retrieved_chunks)
        else:
            results["recall"] = {"metric": "recall", "score": "N/A", "reasoning": "Insufficient data for recall evaluation"}

        # Calculate average score (only from metric dictionaries, not the query string)
        metric_keys = ["correctness", "relevancy", "recall"]
        scores = []
        for key in metric_keys:
            if key in results and isinstance(results[key], dict):
                score = results[key].get("score")
                if score != "N/A" and isinstance(score, (int, float)):
                    scores.append(score)
        results["average_score"] = sum(scores) / len(scores) if scores else 0

        return results

    def _strip_markdown_code_blocks(self, text: str) -> str:
        """Strip markdown code blocks from LLM response"""
        import re
        # Remove ```json ... ``` or ``` ... ``` wrapping
        pattern = r'^```(?:json)?\s*\n?(.*?)\n?```$'
        match = re.match(pattern, text.strip(), re.DOTALL)
        if match:
            return match.group(1).strip()
        return text.strip()

    def _parse_fallback(self, response_text: str, metric_name: str) -> Dict[str, Any]:
        """Fallback parser when JSON parsing fails"""
        import re

        score_match = re.search(r'["\']?score["\']?:\s*(\d)', response_text)
        score = int(score_match.group(1)) if score_match else 0

        return {
            "metric": metric_name,
            "score": score,
            "reasoning": response_text[:200],
            "parse_method": "fallback"
        }


if __name__ == "__main__":
    print("LLMJudge module - use via evaluation script")
